{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a3b62-128b-47f5-ba2c-c0626910ea49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28b0f7e-cf72-4ff0-8ae2-864d8a76405e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "--2023-05-04 06:24:44--  https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt\n",
       "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
       "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
       "HTTP request sent, awaiting response... 200 OK\n",
       "Length: 1115394 (1.1M) [text/plain]\n",
       "Saving to: âtiny-shakespeare.txtâ\n",
       "\n",
       "     0K .......... .......... .......... .......... ..........  4% 2.50M 0s\n",
       "    50K .......... .......... .......... .......... ..........  9% 2.71M 0s\n",
       "   100K .......... .......... .......... .......... .......... 13% 11.0M 0s\n",
       "   150K .......... .......... .......... .......... .......... 18% 15.0M 0s\n",
       "   200K .......... .......... .......... .......... .......... 22% 4.07M 0s\n",
       "   250K .......... .......... .......... .......... .......... 27% 13.1M 0s\n",
       "   300K .......... .......... .......... .......... .......... 32% 14.6M 0s\n",
       "   350K .......... .......... .......... .......... .......... 36% 20.3M 0s\n",
       "   400K .......... .......... .......... .......... .......... 41% 12.5M 0s\n",
       "   450K .......... .......... .......... .......... .......... 45% 37.4M 0s\n",
       "   500K .......... .......... .......... .......... .......... 50% 20.7M 0s\n",
       "   550K .......... .......... .......... .......... .......... 55% 16.4M 0s\n",
       "   600K .......... .......... .......... .......... .......... 59% 20.1M 0s\n",
       "   650K .......... .......... .......... .......... .......... 64% 17.5M 0s\n",
       "   700K .......... .......... .......... .......... .......... 68% 63.7M 0s\n",
       "   750K .......... .......... .......... .......... .......... 73% 65.6M 0s\n",
       "   800K .......... .......... .......... .......... .......... 78% 19.1M 0s\n",
       "   850K .......... .......... .......... .......... .......... 82% 61.0M 0s\n",
       "   900K .......... .......... .......... .......... .......... 87% 70.4M 0s\n",
       "   950K .......... .......... .......... .......... .......... 91% 16.6M 0s\n",
       "  1000K .......... .......... .......... .......... .......... 96% 69.4M 0s\n",
       "  1050K .......... .......... .......... .........            100% 78.2M=0.09s\n",
       "\n",
       "2023-05-04 06:24:44 (11.5 MB/s) - âtiny-shakespeare.txtâ saved [1115394/1115394]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    ":! wget \"https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77de8739-88a3-47b0-a7ac-dbcb11a99bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "-- read and inspect it\n",
    "text <- readFile \"tiny-shakespeare.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12177704-56fc-4cde-83d4-d28a9ab0561d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print $ length text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c9ae1b2-aac5-4873-b8b5-ec429ee8c4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "First Citizen:\n",
       "Before we proceed any further, hear me speak.\n",
       "\n",
       "All:\n",
       "Speak, speak.\n",
       "\n",
       "First Citizen:\n",
       "You are all resolved rather to die than to famish?\n",
       "\n",
       "All:\n",
       "Resolved. resolved.\n",
       "\n",
       "First Citizen:\n",
       "First, you know Caius Marcius is chief enemy to the people.\n",
       "\n",
       "All:\n",
       "We know't, we know't.\n",
       "\n",
       "First Citizen:\n",
       "Let us kill him, and we'll have corn at our own price.\n",
       "Is't a verdict?\n",
       "\n",
       "All:\n",
       "No more talking on't; let it be done: away, away!\n",
       "\n",
       "Second Citizen:\n",
       "One word, good citizens.\n",
       "\n",
       "First Citizen:\n",
       "We are accounted poor citizens, the patricians good.\n",
       "What authority surfeits on would relieve us: if they\n",
       "would yield us but the superfluity, while it were\n",
       "wholesome, we might guess they relieved us humanely;\n",
       "but they think we are too dear: the leanness that\n",
       "afflicts us, the object of our misery, is as an\n",
       "inventory to particularise their abundance; our\n",
       "sufferance is a gain to them Let us revenge this with\n",
       "our pikes, ere we become rakes: for the gods know I\n",
       "speak this in hunger for bread, not in thirst for revenge."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Prelude\n",
    "import qualified Prelude\n",
    "-- let's look at the first 1000 characters\n",
    "putStr $ Prelude.take 1000 text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "febe18f1-8664-4701-a814-129dc536047d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- Here are all the unique characters that occur in this text\n",
    "import Data.List (sort)\n",
    "import qualified Data.Set as S\n",
    "chars = (sort . S.toList . S.fromList) text \n",
    "vocabSize = length chars\n",
    "print chars\n",
    "print vocabSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8999caf-6652-40e4-ac4b-04761c217494",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46,47,47,1,58,46,43,56,43]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"hii there\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- create a mapping from characters to integers\n",
    "import qualified Data.Map as M\n",
    "\n",
    "stoi :: M.Map Char Int\n",
    "stoi = M.fromList $ zip chars [0..]\n",
    "\n",
    "itos :: M.Map Int Char\n",
    "itos = M.fromList $ zip [0..] chars\n",
    "\n",
    "-- encoder: take a string, output a list of integers\n",
    "encode :: String -> [Int]\n",
    "encode = map (stoi M.!)\n",
    "\n",
    "-- decoder: take a list of integers, output a string\n",
    "decode :: [Int] -> String\n",
    "decode = map (itos M.!)\n",
    "\n",
    "print $ encode \"hii there\"\n",
    "print $ (decode . encode) \"hii there\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e899a9d6-50cd-4eee-b5e3-a7209fdf6a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "{-# LANGUAGE OverloadedStrings #-}\n",
    "import Data.Text (Text)\n",
    "import qualified Data.Text as T\n",
    "import Data.Vector (Vector)\n",
    "import qualified Data.Vector as V\n",
    "import qualified Data.Map as M\n",
    "\n",
    "-- create a mapping from characters to integers\n",
    "\n",
    "stoi :: M.Map Char Int\n",
    "stoi = M.fromList $ zip chars [0..]\n",
    "\n",
    "itos :: M.Map Int Char\n",
    "itos = M.fromList $ zip [0..] chars\n",
    "\n",
    "encode :: Text -> [Int]\n",
    "encode = map (stoi M.!) . T.unpack\n",
    "\n",
    "decode :: [Int] -> Text\n",
    "decode = T.pack . map (itos M.!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e306f12-10f7-4c80-b705-a813c6540a47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Shape: [1115394]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DType: Int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "First 1000 characters:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tensor Int64 [1000] [ 18,  47,  56,  57,  58,  1,  15,  47,  58,  47,  64,  43,  52,  10,  0,  14,  43,  44,  53,  56,  43,  1,  61,  43,  1,  54,  56,  53,  41,  43,  43,  42,  1,  39,  52,  63,  1,  44,  59,  56,  58,  46,  43,  56,  6,  1,  46,  43,  39,  56,  1,  51,  43,  1,  57,  54,  43,  39,  49,  8,  0,  0,  13,  50,  50,  10,  0,  31,  54,  43,  39,  49,  6,  1,  57,  54,  43,  39,  49,  8,  0,  0,  18,  47,  56,  57,  58,  1,  15,  47,  58,  47,  64,  43,  52,  10,  0,  37,  53,  59,  1,  39,  56,  43,  1,  39,  50,  50,  1,  56,  43,  57,  53,  50,  60,  43,  42,  1,  56,  39,  58,  46,  43,  56,  1,  58,  53,  1,  42,  47,  43,  1,  58,  46,  39,  52,  1,  58,  53,  1,  44,  39,  51,  47,  57,  46,  12,  0,  0,  13,  50,  50,  10,  0,  30,  43,  57,  53,  50,  60,  43,  42,  8,  1,  56,  43,  57,  53,  50,  60,  43,  42,  8,  0,  0,  18,  47,  56,  57,  58,  1,  15,  47,  58,  47,  64,  43,  52,  10,  0,  18,  47,  56,  57,  58,  6,  1,  63,  53,  59,  1,  49,  52,  53,  61,  1,  15,  39,  47,  59,  57,  1,  25,  39,  56,  41,  47,  59,  57,  1,  47,  57,  1,  41,  46,  47,  43,  44,  1,  43,  52,  43,  51,  63,  1,  58,  53,  1,  58,  46,  43,  1,  54,  43,  53,  54,  50,  43,  8,  0,  0,  13,  50,  50,  10,  0,  35,  43,  1,  49,  52,  53,  61,  5,  58,  6,  1,  61,  43,  1,  49,  52,  53,  61,  5,  58,  8,  0,  0,  18,  47,  56,  57,  58,  1,  15,  47,  58,  47,  64,  43,  52,  10,  0,  24,  43,  58,  1,  59,  57,  1,  49,  47,  50,  50,  1,  46,  47,  51,  6,  1,  39,  52,  42,  1,  61,  43,  5,  50,  50,  1,  46,  39,  60,  43,  1,  41,  53,  56,  52,  1,  39,  58,  1,  53,  59,  56,  1,  53,  61,  52,  1,  54,  56,  47,  41,  43,  8,  0,  21,  57,  5,  58,  1,  39,  1,  60,  43,  56,  42,  47,  41,  58,  12,  0,  0,  13,  50,  50,  10,  0,  26,  53,  1,  51,  53,  56,  43,  1,  58,  39,  50,  49,  47,  52,  45,  1,  53,  52,  5,  58,  11,  1,  50,  43,  58,  1,  47,  58,  1,  40,  43,  1,  42,  53,  52,  43,  10,  1,  39,  61,  39,  63,  6,  1,  39,  61,  39,  63,  2,  0,  0,  31,  43,  41,  53,  52,  42,  1,  15,  47,  58,  47,  64,  43,  52,  10,  0,  27,  52,  43,  1,  61,  53,  56,  42,  6,  1,  45,  53,  53,  42,  1,  41,  47,  58,  47,  64,  43,  52,  57,  8,  0,  0,  18,  47,  56,  57,  58,  1,  15,  47,  58,  47,  64,  43,  52,  10,  0,  35,  43,  1,  39,  56,  43,  1,  39,  41,  41,  53,  59,  52,  58,  43,  42,  1,  54,  53,  53,  56,  1,  41,  47,  58,  47,  64,  43,  52,  57,  6,  1,  58,  46,  43,  1,  54,  39,  58,  56,  47,  41,  47,  39,  52,  57,  1,  45,  53,  53,  42,  8,  0,  35,  46,  39,  58,  1,  39,  59,  58,  46,  53,  56,  47,  58,  63,  1,  57,  59,  56,  44,  43,  47,  58,  57,  1,  53,  52,  1,  61,  53,  59,  50,  42,  1,  56,  43,  50,  47,  43,  60,  43,  1,  59,  57,  10,  1,  47,  44,  1,  58,  46,  43,  63,  0,  61,  53,  59,  50,  42,  1,  63,  47,  43,  50,  42,  1,  59,  57,  1,  40,  59,  58,  1,  58,  46,  43,  1,  57,  59,  54,  43,  56,  44,  50,  59,  47,  58,  63,  6,  1,  61,  46,  47,  50,  43,  1,  47,  58,  1,  61,  43,  56,  43,  0,  61,  46,  53,  50,  43,  57,  53,  51,  43,  6,  1,  61,  43,  1,  51,  47,  45,  46,  58,  1,  45,  59,  43,  57,  57,  1,  58,  46,  43,  63,  1,  56,  43,  50,  47,  43,  60,  43,  42,  1,  59,  57,  1,  46,  59,  51,  39,  52,  43,  50,  63,  11,  0,  40,  59,  58,  1,  58,  46,  43,  63,  1,  58,  46,  47,  52,  49,  1,  61,  43,  1,  39,  56,  43,  1,  58,  53,  53,  1,  42,  43,  39,  56,  10,  1,  58,  46,  43,  1,  50,  43,  39,  52,  52,  43,  57,  57,  1,  58,  46,  39,  58,  0,  39,  44,  44,  50,  47,  41,  58,  57,  1,  59,  57,  6,  1,  58,  46,  43,  1,  53,  40,  48,  43,  41,  58,  1,  53,  44,  1,  53,  59,  56,  1,  51,  47,  57,  43,  56,  63,  6,  1,  47,  57,  1,  39,  57,  1,  39,  52,  0,  47,  52,  60,  43,  52,  58,  53,  56,  63,  1,  58,  53,  1,  54,  39,  56,  58,  47,  41,  59,  50,  39,  56,  47,  57,  43,  1,  58,  46,  43,  47,  56,  1,  39,  40,  59,  52,  42,  39,  52,  41,  43,  11,  1,  53,  59,  56,  0,  57,  59,  44,  44,  43,  56,  39,  52,  41,  43,  1,  47,  57,  1,  39,  1,  45,  39,  47,  52,  1,  58,  53,  1,  58,  46,  43,  51,  1,  24,  43,  58,  1,  59,  57,  1,  56,  43,  60,  43,  52,  45,  43,  1,  58,  46,  47,  57,  1,  61,  47,  58,  46,  0,  53,  59,  56,  1,  54,  47,  49,  43,  57,  6,  1,  43,  56,  43,  1,  61,  43,  1,  40,  43,  41,  53,  51,  43,  1,  56,  39,  49,  43,  57,  10,  1,  44,  53,  56,  1,  58,  46,  43,  1,  45,  53,  42,  57,  1,  49,  52,  53,  61,  1,  21,  0,  57,  54,  43,  39,  49,  1,  58,  46,  47,  57,  1,  47,  52,  1,  46,  59,  52,  45,  43,  56,  1,  44,  53,  56,  1,  40,  56,  43,  39,  42,  6,  1,  52,  53,  58,  1,  47,  52,  1,  58,  46,  47,  56,  57,  58,  1,  44,  53,  56,  1,  56,  43,  60,  43,  52,  45,  43,  8,  0,  0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{-# LANGUAGE QuasiQuotes #-}\n",
    "{-# LANGUAGE TemplateHaskell #-}\n",
    ":ext QuasiQuotes\n",
    ":ext TemplateHaskell\n",
    "\n",
    "import Torch\n",
    "import Torch.Device\n",
    "\n",
    "import qualified Torch.Functional.Internal as TFI\n",
    "import GHC.Int (Int64)\n",
    "\n",
    "device :: Device\n",
    "device = Device CPU 0\n",
    "\n",
    "-- First, we need to convert the encoded text into a Haskell list of Int64 values\n",
    "encodedText :: [Int64]\n",
    "encodedText = fromIntegral <$> encode (T.pack text)\n",
    "\n",
    "-- Now, let's create the Hasktorch tensor from the encodedText list\n",
    "dataTensor :: Tensor\n",
    "dataTensor = asTensor encodedText\n",
    "\n",
    "\n",
    "putStrLn $ \"Shape: \" ++ show (shape dataTensor)\n",
    "putStrLn \"DType: Int64\" -- In Hasktorch, dtype is fixed when using 'asTensor'\n",
    "putStrLn \"First 1000 characters:\"\n",
    "print $ TFI.slice dataTensor 0 0 1000 1\n",
    "-- print $ dataTensor ! [slice|0::1000|]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5866f088-608c-474a-ae8e-42e233fd218f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import GHC.Float (float2Double)\n",
    "\n",
    "-- Calculate the length of the data tensor\n",
    "dataLength :: Int\n",
    "dataLength = length encodedText\n",
    "\n",
    "-- Calculate the split index (90% for training, 10% for validation)\n",
    "splitIndex :: Int\n",
    "splitIndex = round $ float2Double (0.9 * fromIntegral dataLength)\n",
    "\n",
    "-- Split the data into train and validation sets\n",
    "trainValDataPair :: ([Int64], [Int64])\n",
    "trainValDataPair = splitAt splitIndex encodedText\n",
    "\n",
    "trainData = fst trainValDataPair\n",
    "\n",
    "valData = snd trainValDataPair \n",
    "\n",
    "-- Create train and validation tensors\n",
    "trainDataTensor :: Tensor\n",
    "trainDataTensor = asTensor trainData\n",
    "\n",
    "valDataTensor :: Tensor\n",
    "valDataTensor = asTensor valData\n",
    "\n",
    "-- Print the train and validation tensors\n",
    "--putStrLn $ \"Train Data Tensor: \" ++ show trainDataTensor\n",
    "--putStrLn $ \"Validation Data Tensor: \" ++ show valDataTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e2cd290-0c29-4886-83f0-332e23f5ea28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "when input is [18] the target: 47\n",
       "when input is [18,47] the target: 56\n",
       "when input is [18,47,56] the target: 57\n",
       "when input is [18,47,56,57] the target: 58\n",
       "when input is [18,47,56,57,58] the target: 1\n",
       "when input is [18,47,56,57,58,1] the target: 15\n",
       "when input is [18,47,56,57,58,1,15] the target: 47\n",
       "when input is [18,47,56,57,58,1,15,47] the target: 58"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "blockSize :: Int\n",
    "blockSize = 8\n",
    "\n",
    "firstNItems :: Tensor\n",
    "firstNItems = TFI.slice trainDataTensor 0 0 (blockSize + 1) 1\n",
    "\n",
    "\n",
    "trainDataList :: [Int64]\n",
    "trainDataList = [asValue $ select 0 i trainDataTensor :: Int64 | i <- [0 .. (length trainData - 1)]]\n",
    "\n",
    "contextList :: [[Int64]]\n",
    "contextList = [Prelude.take (t + 1) trainDataList | t <- [0 .. (blockSize - 1)]]\n",
    "\n",
    "targetList :: [Int64]\n",
    "targetList = drop 1 trainDataList\n",
    "\n",
    "printPair :: forall {a1} {a2}. (Show a1, Show a2) => (a1, a2) -> IO ()\n",
    "printPair (context, target) = putStrLn $ \"when input is \" ++ show context ++ \" the target: \" ++ show target\n",
    "\n",
    "mapM_ printPair $ zip contextList targetList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54d504cb-28f4-4f15-853a-d92afa7291eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor Float [4] [ 74928.0000   ,  231851.0000   ,  934226.0000   ,  560077.0000   ]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "inputs:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Shape: [4,8]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tensor Int64 [4,8] [[ 56,  6,  0,  24,  43,  58,  1,  61],\n",
       "                    [ 39,  47,  51,  1,  58,  46,  39,  58],\n",
       "                    [ 52,  45,  1,  58,  53,  1,  57,  39],\n",
       "                    [ 43,  47,  52,  45,  1,  46,  53,  50]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "targets:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Shape: [4,8]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tensor Int64 [4,8] [[ 6,  0,  24,  43,  58,  1,  61,  46],\n",
       "                    [ 47,  51,  1,  58,  46,  39,  58,  1],\n",
       "                    [ 45,  1,  58,  53,  1,  57,  39,  63],\n",
       "                    [ 47,  52,  45,  1,  46,  53,  50,  47]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "----"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "here-2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "here -1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0,0),(0,1),(0,2),(0,3),(0,4),(0,5),(0,6),(0,7),(1,0),(1,1),(1,2),(1,3),(1,4),(1,5),(1,6),(1,7),(2,0),(2,1),(2,2),(2,3),(2,4),(2,5),(2,6),(2,7),(3,0),(3,1),(3,2),(3,3),(3,4),(3,5),(3,6),(3,7)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "when input is Tensor Int64 [1,1] [[ 56]] the target: 6\n",
       "when input is Tensor Int64 [1,2] [[ 56,  6]] the target: 0\n",
       "when input is Tensor Int64 [1,3] [[ 56,  6,  0]] the target: 24\n",
       "when input is Tensor Int64 [1,4] [[ 56,  6,  0,  24]] the target: 43\n",
       "when input is Tensor Int64 [1,5] [[ 56,  6,  0,  24,  43]] the target: 58\n",
       "when input is Tensor Int64 [1,6] [[ 56,  6,  0,  24,  43,  58]] the target: 1\n",
       "when input is Tensor Int64 [1,7] [[ 56,  6,  0,  24,  43,  58,  1]] the target: 61\n",
       "when input is Tensor Int64 [1,8] [[ 56,  6,  0,  24,  43,  58,  1,  61]] the target: 46\n",
       "when input is Tensor Int64 [1,1] [[ 39]] the target: 47\n",
       "when input is Tensor Int64 [1,2] [[ 39,  47]] the target: 51\n",
       "when input is Tensor Int64 [1,3] [[ 39,  47,  51]] the target: 1\n",
       "when input is Tensor Int64 [1,4] [[ 39,  47,  51,  1]] the target: 58\n",
       "when input is Tensor Int64 [1,5] [[ 39,  47,  51,  1,  58]] the target: 46\n",
       "when input is Tensor Int64 [1,6] [[ 39,  47,  51,  1,  58,  46]] the target: 39\n",
       "when input is Tensor Int64 [1,7] [[ 39,  47,  51,  1,  58,  46,  39]] the target: 58\n",
       "when input is Tensor Int64 [1,8] [[ 39,  47,  51,  1,  58,  46,  39,  58]] the target: 1\n",
       "when input is Tensor Int64 [1,1] [[ 52]] the target: 45\n",
       "when input is Tensor Int64 [1,2] [[ 52,  45]] the target: 1\n",
       "when input is Tensor Int64 [1,3] [[ 52,  45,  1]] the target: 58\n",
       "when input is Tensor Int64 [1,4] [[ 52,  45,  1,  58]] the target: 53\n",
       "when input is Tensor Int64 [1,5] [[ 52,  45,  1,  58,  53]] the target: 1\n",
       "when input is Tensor Int64 [1,6] [[ 52,  45,  1,  58,  53,  1]] the target: 57\n",
       "when input is Tensor Int64 [1,7] [[ 52,  45,  1,  58,  53,  1,  57]] the target: 39\n",
       "when input is Tensor Int64 [1,8] [[ 52,  45,  1,  58,  53,  1,  57,  39]] the target: 63\n",
       "when input is Tensor Int64 [1,1] [[ 43]] the target: 47\n",
       "when input is Tensor Int64 [1,2] [[ 43,  47]] the target: 52\n",
       "when input is Tensor Int64 [1,3] [[ 43,  47,  52]] the target: 45\n",
       "when input is Tensor Int64 [1,4] [[ 43,  47,  52,  45]] the target: 1\n",
       "when input is Tensor Int64 [1,5] [[ 43,  47,  52,  45,  1]] the target: 46\n",
       "when input is Tensor Int64 [1,6] [[ 43,  47,  52,  45,  1,  46]] the target: 53\n",
       "when input is Tensor Int64 [1,7] [[ 43,  47,  52,  45,  1,  46,  53]] the target: 50\n",
       "when input is Tensor Int64 [1,8] [[ 43,  47,  52,  45,  1,  46,  53,  50]] the target: 47"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{-# LANGUAGE QuasiQuotes #-}\n",
    "{-# LANGUAGE TemplateHaskell #-}\n",
    "{-# LANGUAGE OverloadedLists #-}\n",
    "{-# LANGUAGE DataKinds #-}\n",
    "{-# LANGUAGE GADTs #-}\n",
    "\n",
    "import Torch.Tensor \n",
    "\n",
    "--import Torch.Typed.Functional\n",
    "\n",
    "import Torch.Functional\n",
    "\n",
    "\n",
    "device :: Device\n",
    "device = Device CPU 0\n",
    "\n",
    "torchManualSeed :: forall {a}. Num a => a\n",
    "torchManualSeed = 1337\n",
    "\n",
    "batchSize :: Int\n",
    "batchSize = 4\n",
    "\n",
    "blockSize :: Int\n",
    "blockSize = 8\n",
    "\n",
    "getBatch :: String -> IO (Tensor, Tensor)\n",
    "getBatch split = do\n",
    "    let data' = if split == \"train\" then trainDataTensor else valDataTensor\n",
    "    let dataList'= if split == \"train\" then trainData else valData\n",
    "    gen <- mkGenerator device torchManualSeed\n",
    "    let ix = fst $ randint' 0 ((length dataList') - blockSize) [batchSize] gen\n",
    "    print ix\n",
    "    let indices = [asValue $ select 0 i (toType Int64 ix) :: Int | i <- [0 .. (batchSize - 1)]]\n",
    "    let x = Torch.stack (Dim 0) [TFI.slice data' 0 i (i + blockSize) 1 | i <- indices]\n",
    "    let y = Torch.stack (Dim 0) [TFI.slice data' 0 (i + 1) (i + blockSize + 1) 1 | i <- indices]\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "\n",
    "(xb, yb) <- getBatch \"train\"\n",
    "putStrLn \"inputs:\"\n",
    "putStrLn $ \"Shape: \" ++ show (shape xb)\n",
    "print xb\n",
    "putStrLn \"targets:\"\n",
    "putStrLn $ \"Shape: \" ++ show (shape yb)\n",
    "print yb\n",
    "\n",
    "putStrLn \"----\"\n",
    "\n",
    "batchDims = [0 .. (batchSize - 1)]\n",
    "putStrLn \"here-2\"\n",
    "timeDims = [0 .. (blockSize - 1)]\n",
    "putStrLn \"here -1\"\n",
    "btPairs = [(b, t) | b <- batchDims, t <- timeDims]\n",
    "print btPairs\n",
    "\n",
    "\n",
    "printContextAndTarget :: (Int, Int) -> IO ()\n",
    "printContextAndTarget (b, t) = do\n",
    "  let context = indexSelect' 0 [b] xb\n",
    "  let contextSliced = TFI.slice context 1 0 (t + 1) 1\n",
    "  let targetTensor = indexSelect' 1 [t] (indexSelect' 0 [b] yb)\n",
    "  let target = asValue targetTensor :: Int64\n",
    "  putStrLn $ \"when input is \" ++ show contextSliced ++ \" the target: \" ++ show target\n",
    "\n",
    "\n",
    "\n",
    "mapM_ printContextAndTarget btPairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e142cd-c261-4315-b806-e13989ae7f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import Torch\n",
    "import Torch.NN\n",
    "import Torch.Functional as F\n",
    "\n",
    "-- data BigramLanguageModel {\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bdca00-aa61-414e-bb19-457eb6940e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/* Styles used for the Hoogle display in the pager */\n",
       ".hoogle-doc {\n",
       "display: block;\n",
       "padding-bottom: 1.3em;\n",
       "padding-left: 0.4em;\n",
       "}\n",
       ".hoogle-code {\n",
       "display: block;\n",
       "font-family: monospace;\n",
       "white-space: pre;\n",
       "}\n",
       ".hoogle-text {\n",
       "display: block;\n",
       "}\n",
       ".hoogle-name {\n",
       "color: green;\n",
       "font-weight: bold;\n",
       "}\n",
       ".hoogle-head {\n",
       "font-weight: bold;\n",
       "}\n",
       ".hoogle-sub {\n",
       "display: block;\n",
       "margin-left: 0.4em;\n",
       "}\n",
       ".hoogle-package {\n",
       "font-weight: bold;\n",
       "font-style: italic;\n",
       "}\n",
       ".hoogle-module {\n",
       "font-weight: bold;\n",
       "}\n",
       ".hoogle-class {\n",
       "font-weight: bold;\n",
       "}\n",
       ".get-type {\n",
       "color: green;\n",
       "font-weight: bold;\n",
       "font-family: monospace;\n",
       "display: block;\n",
       "white-space: pre-wrap;\n",
       "}\n",
       ".show-type {\n",
       "color: green;\n",
       "font-weight: bold;\n",
       "font-family: monospace;\n",
       "margin-left: 1em;\n",
       "}\n",
       ".mono {\n",
       "font-family: monospace;\n",
       "display: block;\n",
       "}\n",
       ".err-msg {\n",
       "color: red;\n",
       "font-style: italic;\n",
       "font-family: monospace;\n",
       "white-space: pre;\n",
       "display: block;\n",
       "}\n",
       "#unshowable {\n",
       "color: red;\n",
       "font-weight: bold;\n",
       "}\n",
       ".err-msg.in.collapse {\n",
       "padding-top: 0.7em;\n",
       "}\n",
       ".highlight-code {\n",
       "white-space: pre;\n",
       "font-family: monospace;\n",
       "}\n",
       ".suggestion-warning { \n",
       "font-weight: bold;\n",
       "color: rgb(200, 130, 0);\n",
       "}\n",
       ".suggestion-error { \n",
       "font-weight: bold;\n",
       "color: red;\n",
       "}\n",
       ".suggestion-name {\n",
       "font-weight: bold;\n",
       "}\n",
       "</style><div class=\"suggestion-name\" style=\"clear:both;\">Use newtype instead of data</div><div class=\"suggestion-row\" style=\"float: left;\"><div class=\"suggestion-warning\">Found:</div><div class=\"highlight-code\" id=\"haskell\">data BigramLanguageModelSpec\n",
       "  = BigramLanguageModelSpec {vocabSize :: Int}\n",
       "  deriving (Show, Eq)</div></div><div class=\"suggestion-row\" style=\"float: left;\"><div class=\"suggestion-warning\">Why Not:</div><div class=\"highlight-code\" id=\"haskell\">newtype BigramLanguageModelSpec\n",
       "  = BigramLanguageModelSpec {vocabSize :: Int}\n",
       "  deriving (Show, Eq)</div></div><div class=\"suggestion-name\" style=\"clear:both;\">Use newtype instead of data</div><div class=\"suggestion-row\" style=\"float: left;\"><div class=\"suggestion-warning\">Found:</div><div class=\"highlight-code\" id=\"haskell\">data BigramLanguageModel\n",
       "  = BigramLanguageModel {tokenEmbeddingTable :: Embedding 'Nothing 100 100 'Learned 'D.Float '(D.CPU,\n",
       "                                                                                               0)}\n",
       "  deriving (Show, Generic)</div></div><div class=\"suggestion-row\" style=\"float: left;\"><div class=\"suggestion-warning\">Why Not:</div><div class=\"highlight-code\" id=\"haskell\">newtype BigramLanguageModel\n",
       "  = BigramLanguageModel {tokenEmbeddingTable :: Embedding 'Nothing 100 100 'Learned 'D.Float '(D.CPU,\n",
       "                                                                                               0)}\n",
       "  deriving (Show, Generic)</div></div><div class=\"suggestion-name\" style=\"clear:both;\">Eta reduce</div><div class=\"suggestion-row\" style=\"float: left;\"><div class=\"suggestion-warning\">Found:</div><div class=\"highlight-code\" id=\"haskell\">forward BigramLanguageModel {..} idx\n",
       "  = embed @'Nothing tokenEmbeddingTable idx</div></div><div class=\"suggestion-row\" style=\"float: left;\"><div class=\"suggestion-warning\">Why Not:</div><div class=\"highlight-code\" id=\"haskell\">forward BigramLanguageModel {..}\n",
       "  = embed @'Nothing tokenEmbeddingTable</div></div>"
      ],
      "text/plain": [
       "Line 16: Use newtype instead of data\n",
       "Found:\n",
       "data BigramLanguageModelSpec\n",
       "  = BigramLanguageModelSpec {vocabSize :: Int}\n",
       "  deriving (Show, Eq)\n",
       "Why not:\n",
       "newtype BigramLanguageModelSpec\n",
       "  = BigramLanguageModelSpec {vocabSize :: Int}\n",
       "  deriving (Show, Eq)Line 20: Use newtype instead of data\n",
       "Found:\n",
       "data BigramLanguageModel\n",
       "  = BigramLanguageModel {tokenEmbeddingTable :: Embedding 'Nothing 100 100 'Learned 'D.Float '(D.CPU,\n",
       "                                                                                               0)}\n",
       "  deriving (Show, Generic)\n",
       "Why not:\n",
       "newtype BigramLanguageModel\n",
       "  = BigramLanguageModel {tokenEmbeddingTable :: Embedding 'Nothing 100 100 'Learned 'D.Float '(D.CPU,\n",
       "                                                                                               0)}\n",
       "  deriving (Show, Generic)Line 30: Eta reduce\n",
       "Found:\n",
       "forward BigramLanguageModel {..} idx\n",
       "  = embed @'Nothing tokenEmbeddingTable idx\n",
       "Why not:\n",
       "forward BigramLanguageModel {..}\n",
       "  = embed @'Nothing tokenEmbeddingTable"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "<interactive>:16:14: error:\n    • Variable not in scope: sample :: BigramLanguageModelSpec -> IO BigramLanguageModel\n    • Perhaps you meant ‘D.sample’ (imported from Torch.Typed)\n<interactive>:21:12: error:\n    • Variable not in scope: randInt :: t4 -> (a1, b0) -> IO (Tensor '( 'D.CPU, 0) 'D.Int64 '[8, 10])\n    • Perhaps you meant one of these: ‘D.randint’ (imported from Torch.Typed), ‘D.randn’ (imported from Torch.Typed)\n<interactive>:21:46: error: Data constructor not in scope: Proxy\n<interactive>:9:32: error:\n    • Variable not in scope: sample :: a0 -> IO (Embedding 'Nothing 100 100 'Learned 'D.Float '( 'D.CPU, 0))\n    • Perhaps you meant ‘D.sample’ (imported from Torch.Typed)\n<interactive>:9:87: error: Data constructor not in scope: SNat\n<interactive>:9:99: error: Data constructor not in scope: SNat\n<interactive>:9:111: error: Data constructor not in scope: SDataType\n<interactive>:9:132: error:\n    • Data constructor not in scope: SDevice\n    • Perhaps you meant one of these: ‘D.Device’ (imported from Torch.Device), variable ‘device’ (imported from Torch.Typed.Tensor), variable ‘D.toDevice’ (imported from Torch.Typed)"
     ]
    }
   ],
   "source": [
    "{-# LANGUAGE DataKinds #-}\n",
    "{-# LANGUAGE DeriveGeneric #-}\n",
    "{-# LANGUAGE RecordWildCards #-}\n",
    "{-# LANGUAGE ScopedTypeVariables #-}\n",
    "{-# LANGUAGE TypeApplications #-}\n",
    "\n",
    "\n",
    "import GHC.Generics (Generic)\n",
    "import Torch.Typed.Autograd \n",
    "import Torch.Typed.Tensor \n",
    "import Torch.Typed.NN\n",
    "import Torch.Typed.Functional\n",
    "import qualified Torch.Device as D\n",
    "import qualified Torch.Typed as D\n",
    "\n",
    "data BigramLanguageModelSpec = BigramLanguageModelSpec {\n",
    "    vocabSize :: Int\n",
    "    } deriving (Show, Eq)\n",
    "\n",
    "data BigramLanguageModel = BigramLanguageModel {\n",
    "    tokenEmbeddingTable :: Embedding 'Nothing 100 100 'Learned 'D.Float '(D.CPU, 0)  \n",
    "    } deriving (Show, Generic)\n",
    "\n",
    "instance D.Randomizable BigramLanguageModelSpec BigramLanguageModel where\n",
    "    sample BigramLanguageModelSpec {..} = do\n",
    "        tokenEmbeddingTable <- sample $ LearnedEmbeddingWithRandomInitSpec @'Nothing (SNat @100) (SNat @100) (SDataType @D.Float) (SDevice @(D.CPU, 0))\n",
    "        return $ BigramLanguageModel tokenEmbeddingTable\n",
    "\n",
    "forward :: BigramLanguageModel -> Tensor '(D.CPU, 0) 'D.Int64 '[8, 10] -> Tensor '(D.CPU, 0) 'D.Float '[8, 10, 100]\n",
    "forward BigramLanguageModel {..} idx = embed @'Nothing tokenEmbeddingTable idx\n",
    "\n",
    "main :: IO ()\n",
    "main = do\n",
    "    let vocabSize = 100\n",
    "    model <- sample $ BigramLanguageModelSpec vocabSize\n",
    "\n",
    "    let batchSize = 8\n",
    "    let inputSeqLen = 10\n",
    "\n",
    "    idx <- randInt @('D.Int64, '(D.CPU, 0)) (Proxy @'[8, 10]) (0, vocabSize)\n",
    "    let logits = forward model idx\n",
    "\n",
    "    putStrLn $ \"Logits shape: \" ++ show (shape logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45124ba-933e-475e-94d9-dd64b1d71394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097ed95-891b-43fe-be2f-56fdc4e2ba12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "mimetype": "text/x-haskell",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "9.2.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
